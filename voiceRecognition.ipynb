{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0V3spJp9oHXG",
   "metadata": {
    "id": "0V3spJp9oHXG"
   },
   "source": [
    "------ konec pripojovani souboru z disku"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236735d9-bb42-4378-a4ab-1a73be6e54f3",
   "metadata": {
    "id": "236735d9-bb42-4378-a4ab-1a73be6e54f3"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import src.libs.ikrlib as il\n",
    "import numpy as np\n",
    "from numpy.random import randint\n",
    "import scipy\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844964a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path for data (folder, where are folders target_dev,...)\n",
    "MAIN_PATH=\"data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5e2ada-0377-42a7-889d-5ea334e3ad9a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5931,
     "status": "ok",
     "timestamp": 1713721185189,
     "user": {
      "displayName": "Jan Holáň",
      "userId": "01645984600079185810"
     },
     "user_tz": -120
    },
    "id": "5b5e2ada-0377-42a7-889d-5ea334e3ad9a",
    "outputId": "16f96d52-e5ef-4242-c07f-1aa82a5372a0"
   },
   "outputs": [],
   "source": [
    "# load all files from directories\n",
    "target_dev = list(il.wav16khz2mfcc(MAIN_PATH+'/target_dev/').values())\n",
    "target_train = list(il.wav16khz2mfcc(MAIN_PATH+'/target_train/').values())\n",
    "non_target_dev = list(il.wav16khz2mfcc(MAIN_PATH+'/non_target_dev/').values())\n",
    "non_target_train = list(il.wav16khz2mfcc(MAIN_PATH+'/non_target_train/').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061935dc-ba64-469c-b7a9-6bd20b0bdf4c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1713721185189,
     "user": {
      "displayName": "Jan Holáň",
      "userId": "01645984600079185810"
     },
     "user_tz": -120
    },
    "id": "061935dc-ba64-469c-b7a9-6bd20b0bdf4c",
    "outputId": "1a8af26c-5d7f-4db1-8bba-66339ee48f99"
   },
   "outputs": [],
   "source": [
    "target_train = np.vstack(target_train)\n",
    "non_target_train = np.vstack(non_target_train)\n",
    "\n",
    "dim = target_train.shape[1]\n",
    "dim2 = non_target_train.shape[1]\n",
    "print(target_train.shape)\n",
    "print(dim2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6fe170-5a02-4d38-80de-b3749770a77b",
   "metadata": {
    "id": "ec6fe170-5a02-4d38-80de-b3749770a77b"
   },
   "source": [
    "PCA (mainly overtaken from SUR example project *demo_genderID.py*)\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5676e74c-60db-439b-98bf-582c52357e1f",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "executionInfo": {
     "elapsed": 1237,
     "status": "ok",
     "timestamp": 1713721186421,
     "user": {
      "displayName": "Jan Holáň",
      "userId": "01645984600079185810"
     },
     "user_tz": -120
    },
    "id": "5676e74c-60db-439b-98bf-582c52357e1f",
    "outputId": "b3b13f90-b82f-432e-bcb3-14727c73d6d3"
   },
   "outputs": [],
   "source": [
    "# PCA analysis\n",
    "cov_tot = np.cov(np.vstack([target_train, non_target_train]).T, bias=True)\n",
    "# division was obtained when used 5 last vectors, but still it is bad\n",
    "d, e = scipy.linalg.eigh(cov_tot, subset_by_index=[dim-2, dim-1])\n",
    "\n",
    "target_train_pca = target_train.dot(e)\n",
    "non_target_train_pca = non_target_train.dot(e)\n",
    "plt.plot(non_target_train_pca[:,1], non_target_train_pca[:,0], 'r.', ms=1)\n",
    "plt.plot(target_train_pca[:,1], target_train_pca[:,0], 'b.', ms=1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dfbf6f-de77-4de4-86af-9aa70fa1fa15",
   "metadata": {
    "id": "98dfbf6f-de77-4de4-86af-9aa70fa1fa15"
   },
   "source": [
    "LDA (mainly overtaken from SUR example project *demo_genderID.py*)\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419e39b0-f741-4525-8efa-37280097eb43",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "executionInfo": {
     "elapsed": 539,
     "status": "ok",
     "timestamp": 1713721186956,
     "user": {
      "displayName": "Jan Holáň",
      "userId": "01645984600079185810"
     },
     "user_tz": -120
    },
    "id": "419e39b0-f741-4525-8efa-37280097eb43",
    "outputId": "27d8df2f-4e70-49ea-90a0-0dac6b0627a0"
   },
   "outputs": [],
   "source": [
    "# LDA analysis\n",
    "n_target = len(target_train)\n",
    "n_non_target = len(non_target_train)\n",
    "cov_wc = (n_target*np.cov(target_train.T, bias=True) + n_non_target*np.cov(non_target_train.T, bias=True)) / (n_target + n_non_target)\n",
    "cov_ac = cov_tot - cov_wc\n",
    "d, e = scipy.linalg.eigh(cov_ac, cov_wc, eigvals=(dim-1, dim-1))\n",
    "\n",
    "plt.figure()\n",
    "junk = plt.hist(target_train.dot(e), 40, histtype='step', color='b',density=True)\n",
    "junk = plt.hist(non_target_train.dot(e), 40, histtype='step', color='r',density=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55162108-85d0-460b-a31e-4f073b3b8cc6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1713721186956,
     "user": {
      "displayName": "Jan Holáň",
      "userId": "01645984600079185810"
     },
     "user_tz": -120
    },
    "id": "55162108-85d0-460b-a31e-4f073b3b8cc6",
    "outputId": "b1e9fdc0-8f7b-4c9f-9954-284c86e6ad00"
   },
   "outputs": [],
   "source": [
    "# do tests on LDA\n",
    "# Lets define uniform a-priori probabilities of classes:\n",
    "P_target = 0.5\n",
    "P_non_target = 1 - P_target\n",
    "test_set=non_target_dev\n",
    "\n",
    "score=[]\n",
    "mean_target, cov_target = il.train_gauss(target_train)\n",
    "mean_non_target, cov_non_target = il.train_gauss(non_target_train)\n",
    "for tst in test_set:\n",
    "    ll_target = il.logpdf_gauss(tst, mean_target, cov_target)\n",
    "    ll_non_target = il.logpdf_gauss(tst, mean_non_target, cov_non_target)\n",
    "    score_sum = (sum(ll_target) + np.log(P_target)) - (sum(ll_non_target) + np.log(P_non_target))\n",
    "    score.append(score_sum > 0 if test_set==target_dev else score_sum < 0)\n",
    "print(sum(score) / len(score))\n",
    "\n",
    "# Run recognition with 1-dimensional LDA projected data\n",
    "score=[]\n",
    "mean_target, cov_target = il.train_gauss(target_train.dot(e))\n",
    "mean_non_target, cov_non_target = il.train_gauss(non_target_train.dot(e))\n",
    "for tst in test_set:\n",
    "    ll_target = il.logpdf_gauss(tst.dot(e), mean_target, np.atleast_2d(cov_target))\n",
    "    ll_non_target = il.logpdf_gauss(tst.dot(e), mean_non_target, np.atleast_2d(cov_non_target))\n",
    "    score_sum = (sum(ll_target) + np.log(P_target)) - (sum(ll_non_target) + np.log(P_non_target))\n",
    "    score.append(score_sum > 0 if test_set==target_dev else score_sum < 0)\n",
    "print(sum(score) / len(score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcf6eaf-a071-44e9-a87d-2f33c7019025",
   "metadata": {
    "id": "edcf6eaf-a071-44e9-a87d-2f33c7019025"
   },
   "source": [
    "GMM (mainly overtaken from SUR example project *demo_genderID.py*)\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b842f5f0-d47b-433b-bb45-f185274747fa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10323,
     "status": "ok",
     "timestamp": 1713721197272,
     "user": {
      "displayName": "Jan Holáň",
      "userId": "01645984600079185810"
     },
     "user_tz": -120
    },
    "id": "b842f5f0-d47b-433b-bb45-f185274747fa",
    "outputId": "38fe3531-93c6-4463-d3f8-493eb141eeff"
   },
   "outputs": [],
   "source": [
    "P_target = 0.5\n",
    "P_non_target = 1 - P_target\n",
    "\n",
    "# Decide for number of gaussian mixture components used for the target data\n",
    "num_comp_target = 3\n",
    "MUs_target  = target_train[randint(1, len(target_train), num_comp_target)]\n",
    "COVs_target = [np.var(target_train, axis=0)] * num_comp_target\n",
    "Ws_target = np.ones(num_comp_target) / num_comp_target;\n",
    "\n",
    "\n",
    "# Initialize parameters of non target data\n",
    "num_comp_non_target = 4\n",
    "MUs_non_target  = non_target_train[randint(1, len(non_target_train), num_comp_non_target)]\n",
    "COVs_non_target = [np.var(non_target_train, axis=0)] * num_comp_non_target\n",
    "Ws_non_target   = np.ones(num_comp_non_target) / num_comp_non_target;\n",
    "\n",
    "# Run 30 iterations of EM algorithm to train the two GMMs from target and non target data\n",
    "target_ttl_arr = []\n",
    "non_target_ttl_arr = []\n",
    "\n",
    "for jj in range(100):\n",
    "  [Ws_target, MUs_target, COVs_target, TTL_target] = il.train_gmm(target_train, Ws_target, MUs_target, COVs_target);\n",
    "  [Ws_non_target, MUs_non_target, COVs_non_target, TTL_non_target] = il.train_gmm(non_target_train, Ws_non_target, MUs_non_target, COVs_non_target);\n",
    "  target_ttl_arr.append(TTL_target)\n",
    "  non_target_ttl_arr.append(TTL_non_target)\n",
    "  print('Iteration:', jj, ' Total log-likelihood:', TTL_target, 'for target;', TTL_non_target, 'for non target')\n",
    "\n",
    "# print(target_ttl_arr)\n",
    "# plt.plot(target_ttl_arr,label='Log-likelihood of target train data')\n",
    "# plt.plot(non_target_ttl_arr,label='Log-likelihood of non target train data')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# perform testing\n",
    "score=[]\n",
    "for tst in target_dev:\n",
    "    ll_target = il.logpdf_gmm(tst, Ws_target, MUs_target, COVs_target)\n",
    "    ll_non_target = il.logpdf_gmm(tst, Ws_non_target, MUs_non_target, COVs_non_target)\n",
    "    score_sum = (sum(ll_target) + np.log(P_target)) - (sum(ll_non_target) + np.log(P_non_target))\n",
    "    score.append(score_sum)\n",
    "\n",
    "print(score)\n",
    "# print(sum(score)/len(target_dev))\n",
    "\n",
    "score=[]\n",
    "for tst in non_target_dev:\n",
    "    ll_target = il.logpdf_gmm(tst, Ws_target, MUs_target, COVs_target)\n",
    "    ll_non_target = il.logpdf_gmm(tst, Ws_non_target, MUs_non_target, COVs_non_target)\n",
    "    score_sum = (sum(ll_target) + np.log(P_target)) - (sum(ll_non_target) + np.log(P_non_target))\n",
    "    score.append(score_sum)\n",
    "print(score)\n",
    "\n",
    "# print(sum(score)/len(non_target_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TL0VnlFNZIgf",
   "metadata": {
    "id": "TL0VnlFNZIgf"
   },
   "source": [
    "## Test the GMM on your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GaO0xGYG4t6S",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 48701,
     "status": "ok",
     "timestamp": 1713721245968,
     "user": {
      "displayName": "Jan Holáň",
      "userId": "01645984600079185810"
     },
     "user_tz": -120
    },
    "id": "GaO0xGYG4t6S",
    "outputId": "0df2a5e7-d3d4-4dff-b0d3-fa18471d0413"
   },
   "outputs": [],
   "source": [
    "# insert path to your data here (folder with .wav files)\n",
    "PATH=\"SUR_projekt2023-2024_eval/eval/\"\n",
    "# insert file name for the result printout\n",
    "OUT_FILE=\"voice_gmm\"\n",
    "\n",
    "#======================================\n",
    "#======================================\n",
    "\n",
    "# parsing of the data and running test of the model\n",
    "test_set = il.wav16khz2mfcc(PATH).values()\n",
    "\n",
    "# load file names\n",
    "file_names = []\n",
    "for f in glob(PATH + '/*.wav'):\n",
    "  file_names.append(f.removesuffix('.wav'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2tfH_tsp6JiC",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1695,
     "status": "ok",
     "timestamp": 1713721247658,
     "user": {
      "displayName": "Jan Holáň",
      "userId": "01645984600079185810"
     },
     "user_tz": -120
    },
    "id": "2tfH_tsp6JiC",
    "outputId": "5c9a5366-cbaf-4929-979e-af9b25e085b1"
   },
   "outputs": [],
   "source": [
    "# create out file\n",
    "f = open(OUT_FILE, 'w')\n",
    "\n",
    "sum_of_speaker = 0\n",
    "for (tst,f_name) in zip(test_set,file_names):\n",
    "  ll_target = il.logpdf_gmm(tst, Ws_target, MUs_target, COVs_target)\n",
    "  ll_non_target = il.logpdf_gmm(tst, Ws_non_target, MUs_non_target, COVs_non_target)\n",
    "  soft_score = (sum(ll_target) + np.log(P_target)) - (sum(ll_non_target) + np.log(P_non_target))\n",
    "  hard_decission = int(soft_score > 0)\n",
    "  sum_of_speaker += hard_decission\n",
    "  f.write(f'{f_name.split(\"/\")[len(f_name.split(\"/\"))-1]} {soft_score} {hard_decission}\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34a25f80-8415-4e78-8ec3-8d531db51db0",
   "metadata": {
    "id": "34a25f80-8415-4e78-8ec3-8d531db51db0"
   },
   "source": [
    "NEURAL NETWORK\n",
    "==="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef390181-0cea-4b88-b012-749be1dea3e0",
   "metadata": {
    "id": "ef390181-0cea-4b88-b012-749be1dea3e0"
   },
   "outputs": [],
   "source": [
    "# build simple neural network\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import src.libs.ikrlib as il\n",
    "import scipy\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from glob import glob\n",
    "import src.trainmodel as tm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45878d6-5754-435e-8148-13c19e0880ca",
   "metadata": {
    "id": "c45878d6-5754-435e-8148-13c19e0880ca"
   },
   "outputs": [],
   "source": [
    "# load spectograms of files, also all wav files are shortened for the first 1.5s\n",
    "max_length = 0\n",
    "max_freq = 0\n",
    "\n",
    "start_idx_of_signal = int(16000*1.5)\n",
    "nn_target_dev = []\n",
    "for f in glob(MAIN_PATH+'/target_dev/' + '/*.wav'):\n",
    "    fs,s = il.wavfile.read(f)\n",
    "    s = s[start_idx_of_signal:]\n",
    "    f,t,sg = scipy.signal.spectrogram(s,fs)\n",
    "    max_length = len(t) if len(t) > max_length else max_length\n",
    "    max_freq = len(f) if len(f) > max_freq else max_freq\n",
    "    nn_target_dev.append(sg)\n",
    "\n",
    "nn_target_train = []\n",
    "for f in glob(MAIN_PATH+'/target_train/' + '/*.wav'):\n",
    "    fs,s = il.wavfile.read(f)\n",
    "    s = s[start_idx_of_signal:]\n",
    "    _,t,sg = scipy.signal.spectrogram(s,fs)\n",
    "    max_length = len(t) if len(t) > max_length else max_length\n",
    "    max_freq = len(f) if len(f) > max_freq else max_freq\n",
    "    nn_target_train.append(sg)\n",
    "\n",
    "nn_non_target_dev = []\n",
    "for f in glob(MAIN_PATH+'/non_target_dev/' + '/*.wav'):\n",
    "    fs,s = il.wavfile.read(f)\n",
    "    s = s[start_idx_of_signal:]\n",
    "    _,t,sg = scipy.signal.spectrogram(s,fs)\n",
    "    max_length = len(t) if len(t) > max_length else max_length\n",
    "    max_freq = len(f) if len(f) > max_freq else max_freq\n",
    "    nn_non_target_dev.append(sg)\n",
    "\n",
    "nn_non_target_train = []\n",
    "for f in glob(MAIN_PATH+'/non_target_train/' + '/*.wav'):\n",
    "    fs,s = il.wavfile.read(f)\n",
    "    s = s[start_idx_of_signal:]\n",
    "    _,t,sg = scipy.signal.spectrogram(s,fs)\n",
    "    max_length = len(t) if len(t) > max_length else max_length\n",
    "    max_freq = len(f) if len(f) > max_freq else max_freq\n",
    "    nn_non_target_train.append(sg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9_CPmIAI78tk",
   "metadata": {
    "id": "9_CPmIAI78tk"
   },
   "source": [
    "### NEURAL NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a394871-bef2-4548-a21e-ebacac3212c9",
   "metadata": {
    "id": "5a394871-bef2-4548-a21e-ebacac3212c9"
   },
   "outputs": [],
   "source": [
    "# pad spectograms for each train data (and combine them together)\n",
    "padded_spectrograms = []\n",
    "for data in [nn_non_target_train,nn_target_train]:\n",
    "    for spec in data:\n",
    "        padded_spec = np.pad(spec, ((0, 0), (0, max_length - spec.shape[1])), mode='constant')\n",
    "        padded_spectrograms.append(padded_spec)\n",
    "\n",
    "# convert padded train data to\n",
    "padded_spectrograms = np.array(padded_spectrograms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6852d344-878e-4ef7-9729-446bbedd42c9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1713721254624,
     "user": {
      "displayName": "Jan Holáň",
      "userId": "01645984600079185810"
     },
     "user_tz": -120
    },
    "id": "6852d344-878e-4ef7-9729-446bbedd42c9",
    "outputId": "eae04a40-d87b-4f4a-fd64-34b07a9ccfd1"
   },
   "outputs": [],
   "source": [
    "# compute standard deviation and mean for normalization of spectrograms\n",
    "spec_mean = np.mean(padded_spectrograms)\n",
    "spec_std = np.std(padded_spectrograms)\n",
    "padded_spectrograms = (padded_spectrograms - spec_mean) / spec_std\n",
    "print(f\"Spectogram mean is {spec_mean} and standard deviation is {spec_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a39adfc-f2af-416a-98ea-9fc46f7e75ad",
   "metadata": {
    "id": "1a39adfc-f2af-416a-98ea-9fc46f7e75ad"
   },
   "outputs": [],
   "source": [
    "# create labels (we know that non target and target data goes one by one as we loaded them two cells before)\n",
    "labels_non_target = np.linspace(0,0,len(nn_non_target_train))\n",
    "labels_target = np.linspace(1,1,len(nn_target_train))\n",
    "labels_for_train = np.concatenate((labels_non_target,labels_target),axis=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192cbdb2-4a7c-4124-8773-33902c02300c",
   "metadata": {
    "id": "192cbdb2-4a7c-4124-8773-33902c02300c"
   },
   "outputs": [],
   "source": [
    "# prepare data as input for neural network\n",
    "spectrograms_tensor = torch.Tensor(padded_spectrograms).to(device)\n",
    "labels_tensor = torch.LongTensor(labels_for_train).to(device)\n",
    "dataset = TensorDataset(spectrograms_tensor.unsqueeze(1), labels_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aed187-87eb-4c8e-9ad0-06402f41a157",
   "metadata": {
    "id": "38aed187-87eb-4c8e-9ad0-06402f41a157"
   },
   "outputs": [],
   "source": [
    "# build the model\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self,height,width,num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv3 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(32 * (height // 4) * (width // 4), 128)\n",
    "        self.fc2 = nn.Linear(128, num_classes)  # num_classes is the number of output classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten feature maps\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "_, height, width = padded_spectrograms.shape\n",
    "# Initialize the CNN model\n",
    "# del model\n",
    "model = CNN(height,width,2).to(device)\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8975371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the model if necessary for next training\n",
    "# del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "m37bm8v4pn3s",
   "metadata": {
    "id": "m37bm8v4pn3s"
   },
   "source": [
    "### Train function (best setup lr=0.0001, weight_decay=0.005, grad_clip=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788662ca-f60f-432b-a0b4-017ae5c9361c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8570,
     "status": "ok",
     "timestamp": 1713630218609,
     "user": {
      "displayName": "Jan Holáň",
      "userId": "01645984600079185810"
     },
     "user_tz": -120
    },
    "id": "788662ca-f60f-432b-a0b4-017ae5c9361c",
    "outputId": "ee102e66-26b3-4729-b9d8-4bbb2aa7355c"
   },
   "outputs": [],
   "source": [
    "\n",
    "# define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss(weight=torch.FloatTensor([20/152,132/152])).to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.0001, weight_decay=0.005)\n",
    "\n",
    "# create DataLoader for batch processing\n",
    "batch_size = 32\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "losses_overall = []\n",
    "\n",
    "# training\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    losses = []\n",
    "    for batch_inputs, batch_labels in dataloader:\n",
    "        # forward\n",
    "        outputs = model(batch_inputs)\n",
    "        \n",
    "        # torch.save(model.state_dict(), PATH)\n",
    "        # loss\n",
    "        loss = criterion(outputs, batch_labels)\n",
    "        if epoch > 0 : nn.utils.clip_grad_value_(model.parameters(), 0.0005)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "    losses_overall.append(np.mean(losses))\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {np.mean(losses)}')\n",
    "\n",
    "# plot graph\n",
    "plt.plot(np.arange(0,num_epochs,1),losses_overall)\n",
    "plt.title('Loss function')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2400cf51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save currently trained model\n",
    "# PATH=\"nn_model_0.tn\"\n",
    "# torch.save(model.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf7d3ea",
   "metadata": {},
   "source": [
    "#### Evaluate the model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d59322-5ed9-49e7-ac09-bf856976c955",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1713630562338,
     "user": {
      "displayName": "Jan Holáň",
      "userId": "01645984600079185810"
     },
     "user_tz": -120
    },
    "id": "e1d59322-5ed9-49e7-ac09-bf856976c955",
    "outputId": "d9e9763a-328f-42e6-80dd-c59a0e8547ba"
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "# pad spectograms for each test data\n",
    "ps_target_dev = []\n",
    "for spec in nn_target_dev:\n",
    "        padded_spec = np.pad(spec, ((0, 0), (0, max_length - spec.shape[1])), mode='constant')\n",
    "        ps_target_dev.append(padded_spec)\n",
    "\n",
    "ps_non_target_dev = []\n",
    "for spec in nn_non_target_dev:\n",
    "        padded_spec = np.pad(spec, ((0, 0), (0, max_length - spec.shape[1])), mode='constant')\n",
    "        ps_non_target_dev.append(padded_spec)\n",
    "\n",
    "# convert padded test data to array\n",
    "ps_target_dev = np.array(ps_target_dev)\n",
    "ps_non_target_dev = np.array(ps_non_target_dev)\n",
    "\n",
    "# target dev spectrograms tensor\n",
    "td_tensor = torch.Tensor(ps_target_dev)\n",
    "sum_pred_td = 0\n",
    "for t in td_tensor:\n",
    "    input_spec = t.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    ev = model(input_spec)\n",
    "    pred_class = torch.argmax(ev).to(device)\n",
    "    sum_pred_td += pred_class.int()\n",
    "print(f\"Success rate on target_dev: {sum_pred_td/len(td_tensor)}\")\n",
    "\n",
    "# target dev spectrograms tensor\n",
    "non_td_tensor = torch.Tensor(ps_non_target_dev)\n",
    "sum_pred_td = 0\n",
    "for t in non_td_tensor:\n",
    "    input_spec = t.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    ev = model(input_spec)\n",
    "    pred_class = torch.argmax(ev).to(device)\n",
    "    sum_pred_td += pred_class.int()\n",
    "print(f\"Success rate on non_target_dev: {1-sum_pred_td/len(non_td_tensor)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5737de3c",
   "metadata": {},
   "source": [
    "### Evaluate custom data on trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac299c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# set path for output file\n",
    "OUTPUT=\"data/results/voice_convnn\"\n",
    "# prepare data\n",
    "PATH=\"SUR_projekt2023-2024_eval/eval\"\n",
    "max_length=3517 # DO NOT CHANGE, the max length of an input\n",
    "\n",
    "# ************************************************************\n",
    "\n",
    "# load spectrograms of input wavs from given folder\n",
    "start_idx_of_signal = int(16000*1.5)\n",
    "nn_real_data = []\n",
    "file_names=[]\n",
    "for _,_,files in os.walk(PATH):# glob(PATH + '/*.wav'):\n",
    "    for file in files:\n",
    "        if (file.endswith('.wav')):\n",
    "            file_names.append(file.replace('.wav',''))\n",
    "            # compute the spectrogram\n",
    "            fs,s = il.wavfile.read(os.path.join(PATH,file))\n",
    "            s = s[start_idx_of_signal:]\n",
    "            f,t,sg = scipy.signal.spectrogram(s,fs)\n",
    "            nn_real_data.append(sg[:,:3517])\n",
    "\n",
    "\n",
    "# pad the spectrograms\n",
    "ps_real_data = []\n",
    "for spec in nn_real_data:\n",
    "    padded_spec = np.pad(spec, ((0, 0), (0, max_length - spec.shape[1])), mode='constant')\n",
    "    # padded_spec = (padded_spec - np.mean(padded_spec)) / np.std(padded_spec)\n",
    "    ps_real_data.append(padded_spec)\n",
    "\n",
    "# convert padded spectrograms to an array and then to tensor\n",
    "ps_real_data=np.array(ps_real_data)\n",
    "td_tensor_real_data = torch.Tensor(ps_real_data)\n",
    "\n",
    "# ***************************** go through the data and evaluate them *****************************\n",
    "\n",
    "f = open(OUTPUT,'w')\n",
    "# loop through the data write results to the file\n",
    "f_idx = 0\n",
    "\n",
    "# target dev spectrograms tensor\n",
    "# td_tensor = torch.Tensor(ps_real_data)\n",
    "sum_pred_td = 0\n",
    "for t in td_tensor_real_data:\n",
    "    input_spec = t.unsqueeze(0).unsqueeze(0).to(device)\n",
    "    output = model(input_spec)\n",
    "\n",
    "    f.write(f'{file_names[f_idx]} {output.to('cpu').detach().numpy()[0][output.argmax()]} {output.argmax()}\\n')\n",
    "    sum_pred_td += output.argmax()\n",
    "    f_idx +=1\n",
    "\n",
    "f.close()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
